\documentclass[conference]{IEEEtran}

\newcommand{\thetitle}{Non-Parametric Discrete Mixture Model Recovery via Nonnegative Matrix Factorization}

\input{packages}
\input{definitions}

\title{\vspace{-0.25em}\thetitle}
\author{
{\large{Stefan~Karpinski, John~R.~Gilbert, Elizabeth~M.~Belding}} \vspace{0.25em}\\
Department of Computer Science \\
University of California, Santa Barbara \vspace{0.35em}\\
\textit{\{sgk,gilbert,ebelding\}@cs.ucsb.edu}
}

\bibliographystyle{IEEEtran}

\newcommand{\figurename}{Figure}
\newcommand{\tablename}{Table}

\begin{document}
\maketitle

Mixture modeling expresses probability densities as convex combinations of constituent probability distributions:
\begin{align}\eqn{mixture-model}
  q_i(x) = \sum_{j=1}^r w_{ij} p_j(x),
\end{align}
Here $q_i$ and $p_j$ are density functions, and $w_{ij}$ are nonnegative weights, summing to unity for each $i$.
In classical mixture modeling, the constituent density functions, $p_j$, are assumed to be from some class of parametric distributions.
Various well established algorithms, using expectation minimization (\caps{EM}), can optimally recover the weights, $w_{ij}$, given an observed sample of values from $q_i$.

In certain settings, however, mixture modeling is desirable, but the constituent distributions are neither known in advance, nor can they be assumed to be parametric.
In this work, we demonstrate how, for discrete event spaces, nonnegative matrix factorization (\caps{NMF}) can be effectively used to simultaneously recover both weights and constituent distributions, given a large collection of variably-sized samples from mixtures.
% Our work addresses mixture models over discrete event spaces, but can be applied readily to continuous spaces, since continuous quantities can be discretized, and our techniques then applied to the resulting large discrete spaces. Since our approach makes no parametric assumptions, the results are no less valid.

For discrete event spaces, \Equation{mixture-model} is expressed succinctly as matrix multiplication.
Letting $Q_{ik} = q_i(k)$, $W_{ij} = w_{ij}$, and $P_{jk} = p_j(k)$ we have:
\begin{align}\eqn{mixture-model-matrix}
  Q = WP.
\end{align}
% The matrices are all be row-stochastic.
The problem of inferring both the weights, $w_{ij}$, and constituent distributions, $p_j$, from a collection of mixtures, $q_i$, is equivalent to finding the factors $W$ and $P$ given $Q$.
All three matrices are constrined to be row-stochastic, meaning that all entries are nonnegative, with rows summing to unity.

The problem of finding such a factorization is known as nonnegative matrix factorization.
Such factorizations are not unique, so perfect recovery of $W$ and $P$ cannot generally be achieved.
On the other hand, any exact factorization of $Q$, is an equally valid mixture model for the given data.
Since a variety of \caps{NMF} algorithms have been proposed, this problem is partially solved.
Several difficulties remain, however:
\begin{enumerate}
  \item $Q$ is not known exactly, only a finite sample for each distribution row of $Q$ is observed;
  \item The samples for the rows may not have uniform size;
  \item \caps{NMF} is known to be \caps{NP}-hard; thus, all efficient algorithms are heuristic, and may not yield adequate results.
\end{enumerate}
This list is not exhaustive, and we will address and discuss several further difficulties.

Our motivating application is mixture modeling for traces of network flows, whose distributions of packet sizes and inter-packet intervals seem to be effectively modeled as discretized mixture models, using \caps{NMF}~\cite{Karpinski08}.
In this setting, there are several particularly challenging aspects:
\begin{enumerate}
  \item The distribution of sample sizes is heavy-tailed, having a few very large samples, and many very small samples;
  \item The constituent distributions are not uniformly represented: the most prevalent distribution has much larger average weights than the next, and so on.
\end{enumerate}
We will demonstrate using simulated data why both of these properties make factor recovery particularly difficult.

\begin{figure}[b]
\fig{synthetic-basis}
\begin{center}
\includegraphics[width=3.5in]{synth/test}
\end{center}
\vspace{-0.7em}
\caption{Discrete distributions used to generate synthetic mixtures.}
\vspace{-0.5em}
\end{figure}

\begin{figure}[b]
\fig{synthetic-weights}
\begin{center}
\includegraphics[width=3.5in]{synth/weights}
\end{center}
\vspace{-0.7em}
\caption{Transposed matrix plot of 100 sample weight vectors.}
\vspace{-0.5em}
\end{figure}

To evaluate the effectiveness of \caps{NMF} techniques for discrete mixture model recovery, we use synthetic data, since otherwise the true factors are unknown.
To generate synthetic data, we use saw-tooth patterns as constituent distributions, \caps{PDF}s of which are shown in Figure~1.
These distributions are visually distinctive and not well-approximated by standard parametric distributions.
The Pareto distribution is the classic heavy-tailed distribution, and has been shown to describe the distribution of flow sizes in several network trace studies.
Accordingly, we choose sample sizes for each synthetic mixture from a Pareto distribution.
Our synthetic weight matrices are also generated such that the prevalences of the component distributions\,---\,i.e. the column sums of $W$---\,are Pareto-distributed. Figure 2 is a matrix plot of sample rows of randomly generated weights (transposed to save space).

\begin{figure}[t]
\begin{center}
\subfloat[Lee \& Seung, Euclidean Algorithm]
{\includegraphics[width=3.5in]{synth/Q_euclidean}}
\subfloat[Lee \& Seung, Kullback-Leibler Algorithm]
{\includegraphics[width=3.5in]{synth/Q_divergence}}
\subfloat[Kim \& Park, ANLS Algorithm]
{\includegraphics[width=3.5in]{synth/Q_anls}}
\end{center}
\caption{Recovered $P$ distributions for standard \caps{NMF} algorithms, with random initialization and perfect knowledge of $Q$.}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=3.5in]{synth/Q_ki}
\end{center}
\vspace{-0.7em}
\caption{$P_0$ computed using SVD/$k$-means initialization.}
\vspace{-0.5em}
\end{figure}

In our summary, we find that none of the existing \caps{NMF} algorithms, can accurately recover the constituent distributions used to generate synthetic mixtures.
The recovered rows of $P$ for Lee and Seung's multiplicative update algorithms~\cite{Lee01}, and Kim and Park's alternating nonnegative least squares (\caps{ANLS}) algorithm~\cite{Kim08} are shown in Figure~3, dramatically illustrating their failure to recover the original distributions in Figure~1.
It is well known that these algorithms do not find a globally optimum factorization, but rather converge to a local optimum.
The quality of the end result is largely dependent on the initial conditions of the algorithm, which in these instances, were the typical random nonnegative matrices.
To address this problem, we use a variation of the most promising initialization technique proposed by Langville~et~al.~\cite{Langville07}:
\begin{enumerate}
  \item Let $Q=USV'$, the singular value decomposition (\caps{SVD}),
  \item Use $k$-means to find $r$ clusters of columns in $V$,
  \item Let $W_0$ be corresponding column centroids in $Q$,
  \item Let $P_0$ be nonnegative minimizing $\norm{Q - W_0 P_0}_F$.
\end{enumerate}
Figure~4 shows that even without any further refinement, this initialization technique already recovers the overall shape of $P$ remarkably well.
Although this recovery appears visually close to optimal, there are differences in shape which limit the quality of the approximation $W_0 P_0$.
Intuitively, however, this initialization sits on a ridge above a deep basin, where the perfect recovery lies.
If care is not taken, however, \caps{NMF} algorithms may descend the wrong side of the ridge, moving away from the optimal recovery rather than toward it.

\begin{figure}[t]
\begin{center}
\includegraphics[width=3.5in]{synth/Q_nmf}
\end{center}
\vspace{-0.7em}
\caption{Perfect recovery of $P$ using our meta-algorithm.}
\vspace{-0.5em}
\end{figure}

Through a combination of mathematical insight, intuition and trial and error, we have found that applying the \caps{ANLS} algorithm for a few dozens of iterations, followed by the Kullback-Leibler algorithm, followed finally by the Euclidean algorithm results in arbitrarily perfect recovery, limited only by how many iterations of the Euclidean algorithm one is willing to wait for. The results of this meta-algorithm are shown in Figure~5.

The above results all assume perfect knowledge of $Q$.
In real situations, where discrete mixture models must be recovered from experimental data, there is highly imperfect knowledge of $Q$.
Only a finite sample of values from each $q_i$ distribution are observed.
These samples may be of different sizes, and if in many situations, the vast majority of the samples are very small, many even consisting of only a single value.
Can the basis matrix, $P$, still be recovered under such circumstances?
In fact, the meta-algorithm described above does recover $P$ quite well, so long as care is taken in scaling the rows of the matrices to which the \caps{NMF} algorithms are applied.

\bibliography{IEEE,references}

\end{document}
