\documentclass[conference]{IEEEtran}

\newcommand{\thetitle}{Non-Parametric Discrete Mixture Model Recovery via Nonnegative Matrix Factorization}

\input{packages}
\input{definitions}

\title{\vspace{-0.25em}\thetitle}
\author{
{\large{Stefan~Karpinski, John~R.~Gilbert, Elizabeth~M.~Belding}} \vspace{0.25em}\\
Department of Computer Science \\
University of California, Santa Barbara \vspace{0.35em}\\
\textit{\{sgk,gilbert,ebelding\}@cs.ucsb.edu}
}

\bibliographystyle{IEEEtran}

\newcommand{\figurename}{Figure}
\newcommand{\tablename}{Table}

\begin{document}
\maketitle

Mixture modeling expresses probability densities as convex combinations of constituent probability distributions:
\begin{align}\eqn{mixture-model}
  q_i(x) = \sum_{j=1}^r w_{ij} p_j(x),
\end{align}
Here $q_i$ and $p_j$ are density functions, and $w_{ij}$ are nonnegative weights, summing to unity for each $i$.
In classical mixture modeling, the constituent density functions, $p_j$, are assumed to be from some class of parametric distributions.
Various well established algorithms, using expectation minimization (\caps{EM}), can optimally recover the weights, $w_{ij}$, given an observed sample of values from $q_i$.

In certain settings, however, mixture modeling is desirable, but the constituent distributions are neither known in advance, nor can they be assumed to be parametric.
In this work, we demonstrate how, for discrete event spaces, nonnegative matrix factorization (\caps{NMF}) can be effectively used to simultaneously recover both weights and constituent distributions, given a large collection of variably-sized samples from mixtures.
% Our work addresses mixture models over discrete event spaces, but can be applied readily to continuous spaces, since continuous quantities can be discretized, and our techniques then applied to the resulting large discrete spaces. Since our approach makes no parametric assumptions, the results are no less valid.

For discrete event spaces, \Equation{mixture-model} is expressed succinctly as matrix multiplication.
Letting $Q_{ik} = q_i(k)$, $W_{ij} = w_{ij}$, and $P_{jk} = p_j(k)$ we have:
\begin{align}\eqn{mixture-model-matrix}
  Q = WP.
\end{align}
% The matrices are all be row-stochastic.
The problem of inferring both the weights, $w_{ij}$, and constituent distributions, $p_j$, from a collection of mixtures, $q_i$, is equivalent to finding the factors $W$ and $P$ given $Q$.
All three matrices are constrined to be row-stochastic, meaning that all entries are nonnegative, with rows summing to unity.

The problem of finding such a factorization is known as nonnegative matrix factorization.
Such factorizations are not unique, so perfect recovery of $W$ and $P$ cannot generally be achieved.
On the other hand, any exact factorization of $Q$, is an equally valid mixture model for the given data.
Since a variety of \caps{NMF} algorithms have been proposed, this problem is partially solved.
Several difficulties remain, however:
\begin{enumerate}
  \item $Q$ is not known exactly, only a finite sample for each distribution row of $Q$ is observed;
  \item The samples for the rows may not have uniform size;
  \item \caps{NMF} is known to be \caps{NP}-hard; thus, all efficient algorithms are heuristic, and may not yield adequate results.
\end{enumerate}
This list is not exhaustive, and we will address and discuss several further difficulties.

Our motivating application is mixture modeling for traces of network flows, whose distributions of packet sizes and inter-packet intervals can be effectively modeled as discretized mixture models, using \caps{NMF}~\cite{Karpinski08}.
In this setting, there are several particularly challenging aspects:
\begin{enumerate}
  \item The distribution of sample sizes is heavy-tailed, having a few very large samples, and many very small samples, with most flows having only one to three packets.
  \item The constituent distributions are not uniformly represented: the most prevalent distribution has much larger weights on average than the next, and so on.
\end{enumerate}
We will demonstrate using simulated data why both of these properties make factor recovery particularly difficult.

\begin{figure}[b]
\fig{synthetic-basis}
\begin{center}
\includegraphics[width=3.5in]{synth/test}
\end{center}
\vspace{-0.7em}
\caption{Discrete distributions used to generate synthetic mixtures.}
\vspace{-0.5em}
\end{figure}

\begin{figure}[b]
\fig{synthetic-weights}
\begin{center}
\includegraphics[width=3.5in]{synth/weights}
\end{center}
\vspace{-0.7em}
\caption{Transposed matrix plot of 100 sample random weights.}
\vspace{-0.5em}
\end{figure}

For synthetic data, we use saw-tooth patterns as constituent distributions, as illustrated in Figure~1.
These distributions are visually distinctive, and not well-approximated by standard parametric distributions.
The Pareto distribution is the classic heavy-tailed distribution, and has been shown to describe the distribution of flow sizes in several network trace studies.
Accordingly, we choose sample sizes for each synthetic mixture from a Pareto distribution.
Our synthetic weight matrices are also generated such that the prevalences of the component distributions\,---\,i.e. the column sums of $W$---\,are Pareto-distributed. Figure 2 is a matrix plot of sample rows of randomly generated weights, transposed to save space.

In summary, we find that none of the standard \caps{NMF} algorithms, such as Lee and Seung's multiplicative update rules~\cite{Lee01}, or Kim and Park's alternating nonnegative least squares (\caps{ANLS}) algorithm~\cite{Kim08}.

% The first challenge is troubling because even if $Q$ can be factored exactly with small rank, if there are many small samples, the rank of the sample matrix, $S$, will be large, and its singular values will not 

\bibliography{IEEE,references}

\end{document}
